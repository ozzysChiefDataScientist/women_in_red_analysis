{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives: \n",
    "1) Gather names of people whose Wikipedia articles have been nominated for deletion\n",
    "\n",
    "2) Gather free text discussion about why those articles were nominated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "from bs4 import BeautifulSoup\n",
    "import boto3\n",
    "import config as cfg\n",
    "import datetime\n",
    "import io\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_BUCKET = 'afd-scraped'\n",
    "PREFIX = \"daily_afd_log/2023\"\n",
    "OUTPUT_BUCKET = 'women-in-red-intermediary'\n",
    "OUTPUT_FILE = 'afd_names_and_discussion.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3',\n",
    "                    region_name='us-east-1',\n",
    "                    aws_access_key_id=cfg.aws_reader['accessCode'],\n",
    "                    aws_secret_access_key=cfg.aws_reader['secretCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "boto3.resources.factory.s3.ServiceResource"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_afd_containers(bs4_result_set):\n",
    "    \"\"\"\n",
    "    Extracts the text and tags of the specified type in a BeautifulSoup result set.\n",
    "\n",
    "    Args:\n",
    "        bs4_result_set (bs4.element.ResultSet): A result set containing h3 tags.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of tag names and their contents, where each value is a list of\n",
    "              BeautifulSoup tags representing the contents between the corresponding tags.\n",
    "\n",
    "    Example:\n",
    "        >>> h3_tags = soup.find_all(\"h3\")\n",
    "        >>> h3_containers = create_afd_containers(h3_tags)\n",
    "        >>> print(h3_containers[\"Section 1\"])\n",
    "        [<p>Some text</p>, <ul><li>Item 1</li><li>Item 2</li></ul>, <p>More text</p>]\n",
    "    \"\"\"\n",
    "\n",
    "    afd_containers = {}\n",
    "    TEXT_TO_REMOVE = \"[edit]\"\n",
    "\n",
    "    for i, current_tag in enumerate(bs4_result_set):\n",
    "        clean_object_name = get_afd_article_name(current_tag)\n",
    "        if clean_object_name != \"Not Found\":\n",
    "            afd_containers[clean_object_name] = str(current_tag)\n",
    "\n",
    "    return afd_containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_afd_article_name(div_tag):\n",
    "    \"\"\"\n",
    "    Returns the name of the AFD (Articles for Deletion) article if found in the provided div tag object.\n",
    "    In historical data we have seen that the name of the article is found in the <span> tag\n",
    "    with class =\"mw-headline\"\n",
    "\n",
    "    Parameters:\n",
    "        div_tag (bs4.element.Tag): The div tag object.\n",
    "\n",
    "    Returns:\n",
    "        str: The name of the AFD article if found, otherwise returns 'Not Found'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        article_name = div_tag.find_all('span', class_=\"mw-headline\")[0]['id']\n",
    "        article_name = article_name.replace(\"_\",\" \")\n",
    "        return article_name\n",
    "    except:\n",
    "        return 'Not Found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_afd_result(div_tag_str):\n",
    "    \"\"\"\n",
    "    Extracts and standardizes the outcome of the AFD (Articles for Deletion) discussions\n",
    "    from the HTML code of a div tag.\n",
    "    In historical data we've found the outcome stored in a sentence \"The result was <b>[OUTCOME]</b>\"\n",
    "\n",
    "    Parameters:\n",
    "        div_tag_str (str): The HTML code of a div tag.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The standardized result of the AFD if found, otherwise returns None.\n",
    "    \"\"\"\n",
    "    \n",
    "    result = re.search(r\"The result was <b>([A-Za-z0-9_ ]+)</b>\", div_tag_str)\n",
    "    standardization_lookup = {\"withdrew\": \"withdrawn\"}\n",
    "\n",
    "    try:\n",
    "        result_term = result.group(1).lower()\n",
    "        if result_term in standardization_lookup.keys():\n",
    "            return standardization_lookup.get(result_term)\n",
    "        else:\n",
    "            return result_term\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_people_metadata_from_logs(logs_df, s3_bucket):\n",
    "    '''\n",
    "    Given a Pandas DataFrame of log files and an S3 bucket name, \n",
    "    extracts metadata about people mentioned in the logs.\n",
    "    \n",
    "    :param logs_df: A Pandas DataFrame containing a column of S3 object keys.\n",
    "    :type logs_df: pandas.DataFrame\n",
    "    :param s3_bucket: The name of the S3 bucket containing the log files.\n",
    "    :type s3_bucket: str\n",
    "    :return: A Pandas DataFrame containing metadata about people mentioned in the logs.\n",
    "    :rtype: pandas.DataFrame\n",
    "    '''\n",
    "    \n",
    "    people_metadata = None\n",
    "\n",
    "    for file in logs_df['file_name'].values:\n",
    "        print(file)\n",
    "\n",
    "        object_content = read_s3_file(s3_bucket, file)\n",
    "        div_tags = get_div_tags(object_content)\n",
    "        \n",
    "        afd_content = create_afd_containers(div_tags)\n",
    "\n",
    "        for name in afd_content.keys():\n",
    "                \n",
    "            results = pd.DataFrame(identify_people(name), index=[0])\n",
    "            results['file_name'] = file\n",
    "            results['discussion'] = afd_content.get(name) \n",
    "            results['afd_result'] = get_afd_result(afd_content.get(name))\n",
    "            \n",
    "            if people_metadata is None:\n",
    "                people_metadata = results\n",
    "            else:\n",
    "                people_metadata = pd.concat([people_metadata, results])\n",
    "                \n",
    "    return people_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_s3_files(BUCKET, PREFIX):\n",
    "    \"\"\"\n",
    "    This function takes in the name of an Amazon S3 bucket and a prefix for an S3 key \n",
    "    and returns a list of S3 object keys that match the given prefix.\n",
    "\n",
    "    :param BUCKET: A string representing the name of an S3 bucket\n",
    "    :type BUCKET: str\n",
    "    :param PREFIX: A string representing the prefix to search for in the S3 objects' keys\n",
    "    :type PREFIX: str\n",
    "\n",
    "    :return: A list of S3 object keys that match the given prefix\n",
    "    :rtype: list[str]\n",
    "    \n",
    "    Example Usage:\n",
    "    >>> s3_files = get_list_of_s3_files('my-s3-bucket', 'path/to/my/files/')\n",
    "    >>> print(s3_files)\n",
    "    ['path/to/my/files/file1.txt', 'path/to/my/files/file2.txt', 'path/to/my/files/file3.txt']\n",
    "    \"\"\"\n",
    "    bucket = s3.Bucket(BUCKET)\n",
    "    objects = bucket.objects.filter(Prefix=PREFIX)\n",
    "    return [obj.key for obj in objects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_div_tags(html_page):\n",
    "    \"\"\"\n",
    "    Parses an HTML page and returns a list of all the <div> tags with class \"boilerplate afd vfd xfd-closed archived\"\n",
    "    found in it.\n",
    "\n",
    "    Args:\n",
    "        html_page (str): The HTML page to parse.\n",
    "\n",
    "    Returns:\n",
    "        A list of BeautifulSoup Tag objects, each representing an <h3> tag found in the HTML page.\n",
    "\n",
    "    Raises:\n",
    "        None.\n",
    "\n",
    "    Example:\n",
    "        >>> html_page = '<html><body><h1>Title</h1><h3>First article</h3><p>Some text.</p><h3>Second article</h3><p>More text.</p></body></html>'\n",
    "        >>> parse_h3_tags(html_page)\n",
    "        [<h3>First article</h3>, <h3>Second article</h3>]\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "    div_tags = soup.find_all(\"div\", class_='boilerplate afd vfd xfd-closed archived')\n",
    "    return div_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_recent_log(object_df):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with the most recent 'scrape_date' for each 'log_date'.\n",
    "\n",
    "    :param object_df: pandas.DataFrame\n",
    "        A DataFrame with columns 'file_name', 'scrape_date', and 'log_date'.\n",
    "    :return: pandas.DataFrame\n",
    "        A DataFrame with columns 'log_date' and 'scrape_date', where each row\n",
    "        corresponds to the most recent 'scrape_date' for each 'log_date' in the input DataFrame.\n",
    "    \"\"\"\n",
    "    max_scrape_date = object_df.groupby('log_date')['scrape_date'].max().reset_index()\n",
    "    return max_scrape_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_people(afd_article_name):\n",
    "    \"\"\"\n",
    "    Identify people in an AFD (Articles for Deletion) article name using Named Entity Recognition (NER) provided by the\n",
    "    spaCy library. Returns a dictionary with information about the identified entities.\n",
    "\n",
    "    :param afd_article_name: The name of the AFD article to analyze.\n",
    "    :type afd_article_name: str\n",
    "    :return: A dictionary with information about the identified entities.\n",
    "    :rtype: dict\n",
    "\n",
    "    The dictionary contains the following keys:\n",
    "        - 'entity': The name of the AFD article that was analyzed.\n",
    "        - 'found_person': A boolean value indicating whether at least one person was identified in the article name.\n",
    "        - 'num_entities': The number of unique entity types identified in the article name.\n",
    "        - 'is_multiple_entity_types': A boolean value indicating whether more than one entity type was identified in the\n",
    "                                       article name.\n",
    "\n",
    "    Example usage:\n",
    "    >>> identify_people(\"Wikipedia:Articles for deletion/John Doe\")\n",
    "    {'entity': 'Wikipedia:Articles for deletion/John Doe',\n",
    "     'found_person': True,\n",
    "     'num_entities': 1,\n",
    "     'is_multiple_entity_types': False}\n",
    "    \"\"\"\n",
    "    doc = nlp(afd_article_name)\n",
    "\n",
    "    FOUND_PERSON = False \n",
    "    MULTIPLE_ENTITY_TYPES = False\n",
    "\n",
    "    unique_entity_labels = len(set([entity.label_ for entity in doc.ents])) \n",
    "\n",
    "    if any(entity.label_==\"PERSON\" for entity in doc.ents):\n",
    "        FOUND_PERSON = True\n",
    "    if unique_entity_labels>1 and any(entity.label_==\"PERSON\" for entity in doc.ents):\n",
    "        MULTIPLE_ENTITY_TYPES = True\n",
    "\n",
    "    return {'entity': afd_article_name, 'found_person': FOUND_PERSON, 'num_entities': unique_entity_labels,\n",
    "            'is_multiple_entity_types': MULTIPLE_ENTITY_TYPES}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_s3_file(s3_reader, bucket_name, file_key):\n",
    "    \"\"\"\n",
    "    Reads the contents of a file stored on S3.\n",
    "\n",
    "    :param bucket_name: The name of the S3 bucket.\n",
    "    :type bucket_name: str\n",
    "    :param file_key: The unique key of the file in the S3 bucket.\n",
    "    :type file_key: str\n",
    "    :return: The contents of the file as a string.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    s3_object = s3_reader.Object(bucket_name, file_key)\n",
    "    object_content = s3_object.get()['Body'].read().decode('utf-8')\n",
    "    return object_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_primary_key(df, primary_key_col):\n",
    "    \"\"\"\n",
    "    Checks if a specified column or set of columns constitutes a primary key for a given pandas dataframe. \n",
    "\n",
    "    :param df: Pandas dataframe to be checked for a primary key.\n",
    "    :type df: pandas.DataFrame\n",
    "    :param primary_key_col: Name or list of names of column(s) to be checked for being a primary key.\n",
    "    :type primary_key_col: str or list[str]\n",
    "    :raises AssertionError: If the specified column or set of columns is not a primary key for the given dataframe.\n",
    "    :return: None\n",
    "\n",
    "    Example Usage:\n",
    "    >>> import pandas as pd\n",
    "    >>> data = {'Name': ['John', 'Alex', 'Mike', 'John'], 'Age': [24, 26, 27, 24], 'Gender': ['M', 'M', 'M', 'M']}\n",
    "    >>> df = pd.DataFrame(data)\n",
    "    >>> test_primary_key(df, 'Name')\n",
    "    AssertionError: Name is not the primary key\n",
    "    \"\"\"\n",
    "    try:\n",
    "        assert any(df[primary_key_col].duplicated())==False \n",
    "    except:\n",
    "        raise AssertionError(f'{primary_key_col} is not the primary key')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect a list of Article for Deletion logs in this bucket with desired prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = get_list_of_s3_files(INPUT_BUCKET, PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_pd = pd.DataFrame({\"file_name\": objects})\n",
    "objects_pd['scrape_date'] = objects_pd['file_name'].apply(lambda x: pd.to_datetime(x.split(\"/\")[1]))\n",
    "objects_pd['log_date'] = objects_pd['file_name'].apply(lambda x: x.split(\"/\")[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>scrape_date</th>\n",
       "      <th>log_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_21.txt</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2022_December_21.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_22.txt</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2022_December_22.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_23.txt</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2022_December_23.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_24.txt</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2022_December_24.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_25.txt</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2022_December_25.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>daily_afd_log/2023-05-27/2023_May_23.txt</td>\n",
       "      <td>2023-05-27</td>\n",
       "      <td>2023_May_23.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>daily_afd_log/2023-05-27/2023_May_24.txt</td>\n",
       "      <td>2023-05-27</td>\n",
       "      <td>2023_May_24.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>daily_afd_log/2023-05-27/2023_May_25.txt</td>\n",
       "      <td>2023-05-27</td>\n",
       "      <td>2023_May_25.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>daily_afd_log/2023-05-27/2023_May_26.txt</td>\n",
       "      <td>2023-05-27</td>\n",
       "      <td>2023_May_26.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>daily_afd_log/2023-05-27/2023_May_27.txt</td>\n",
       "      <td>2023-05-27</td>\n",
       "      <td>2023_May_27.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>825 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         file_name scrape_date  \\\n",
       "0    daily_afd_log/2023-01-01/2022_December_21.txt  2023-01-01   \n",
       "1    daily_afd_log/2023-01-01/2022_December_22.txt  2023-01-01   \n",
       "2    daily_afd_log/2023-01-01/2022_December_23.txt  2023-01-01   \n",
       "3    daily_afd_log/2023-01-01/2022_December_24.txt  2023-01-01   \n",
       "4    daily_afd_log/2023-01-01/2022_December_25.txt  2023-01-01   \n",
       "..                                             ...         ...   \n",
       "820       daily_afd_log/2023-05-27/2023_May_23.txt  2023-05-27   \n",
       "821       daily_afd_log/2023-05-27/2023_May_24.txt  2023-05-27   \n",
       "822       daily_afd_log/2023-05-27/2023_May_25.txt  2023-05-27   \n",
       "823       daily_afd_log/2023-05-27/2023_May_26.txt  2023-05-27   \n",
       "824       daily_afd_log/2023-05-27/2023_May_27.txt  2023-05-27   \n",
       "\n",
       "                 log_date  \n",
       "0    2022_December_21.txt  \n",
       "1    2022_December_22.txt  \n",
       "2    2022_December_23.txt  \n",
       "3    2022_December_24.txt  \n",
       "4    2022_December_25.txt  \n",
       "..                    ...  \n",
       "820       2023_May_23.txt  \n",
       "821       2023_May_24.txt  \n",
       "822       2023_May_25.txt  \n",
       "823       2023_May_26.txt  \n",
       "824       2023_May_27.txt  \n",
       "\n",
       "[825 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We captured snapshots of Articles for Deletion logs on multiple dates, so let's filter to the most recent snapshot for a given log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_log = get_most_recent_log(objects_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_pd = objects_pd.merge(most_recent_log, on = ['log_date','scrape_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>scrape_date</th>\n",
       "      <th>log_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_21.txt</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2022_December_21.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daily_afd_log/2023-01-02/2022_December_22.txt</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>2022_December_22.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>daily_afd_log/2023-01-03/2022_December_23.txt</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>2022_December_23.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>daily_afd_log/2023-01-04/2022_December_24.txt</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>2022_December_24.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>daily_afd_log/2023-01-05/2022_December_25.txt</td>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>2022_December_25.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>daily_afd_log/2023-05-27/2023_May_23.txt</td>\n",
       "      <td>2023-05-27</td>\n",
       "      <td>2023_May_23.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>daily_afd_log/2023-05-27/2023_May_24.txt</td>\n",
       "      <td>2023-05-27</td>\n",
       "      <td>2023_May_24.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>daily_afd_log/2023-05-27/2023_May_25.txt</td>\n",
       "      <td>2023-05-27</td>\n",
       "      <td>2023_May_25.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>daily_afd_log/2023-05-27/2023_May_26.txt</td>\n",
       "      <td>2023-05-27</td>\n",
       "      <td>2023_May_26.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>daily_afd_log/2023-05-27/2023_May_27.txt</td>\n",
       "      <td>2023-05-27</td>\n",
       "      <td>2023_May_27.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         file_name scrape_date  \\\n",
       "0    daily_afd_log/2023-01-01/2022_December_21.txt  2023-01-01   \n",
       "1    daily_afd_log/2023-01-02/2022_December_22.txt  2023-01-02   \n",
       "2    daily_afd_log/2023-01-03/2022_December_23.txt  2023-01-03   \n",
       "3    daily_afd_log/2023-01-04/2022_December_24.txt  2023-01-04   \n",
       "4    daily_afd_log/2023-01-05/2022_December_25.txt  2023-01-05   \n",
       "..                                             ...         ...   \n",
       "97        daily_afd_log/2023-05-27/2023_May_23.txt  2023-05-27   \n",
       "98        daily_afd_log/2023-05-27/2023_May_24.txt  2023-05-27   \n",
       "99        daily_afd_log/2023-05-27/2023_May_25.txt  2023-05-27   \n",
       "100       daily_afd_log/2023-05-27/2023_May_26.txt  2023-05-27   \n",
       "101       daily_afd_log/2023-05-27/2023_May_27.txt  2023-05-27   \n",
       "\n",
       "                 log_date  \n",
       "0    2022_December_21.txt  \n",
       "1    2022_December_22.txt  \n",
       "2    2022_December_23.txt  \n",
       "3    2022_December_24.txt  \n",
       "4    2022_December_25.txt  \n",
       "..                    ...  \n",
       "97        2023_May_23.txt  \n",
       "98        2023_May_24.txt  \n",
       "99        2023_May_25.txt  \n",
       "100       2023_May_26.txt  \n",
       "101       2023_May_27.txt  \n",
       "\n",
       "[102 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_primary_key(objects_pd, 'log_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The H3 tag only contains the name of the person or organization associated with the article nominated for deletion. Let's collect the articles for deletion discussion, found between H3 tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily_afd_log/2023-01-01/2022_December_21.txt\n",
      "daily_afd_log/2023-01-02/2022_December_22.txt\n",
      "daily_afd_log/2023-01-03/2022_December_23.txt\n",
      "daily_afd_log/2023-01-04/2022_December_24.txt\n",
      "daily_afd_log/2023-01-05/2022_December_25.txt\n",
      "daily_afd_log/2023-01-06/2022_December_26.txt\n",
      "daily_afd_log/2023-01-07/2022_December_27.txt\n",
      "daily_afd_log/2023-01-08/2022_December_28.txt\n",
      "daily_afd_log/2023-01-09/2022_December_29.txt\n",
      "daily_afd_log/2023-01-10/2022_December_30.txt\n",
      "daily_afd_log/2023-01-11/2022_December_31.txt\n",
      "daily_afd_log/2023-01-12/2023_January_1.txt\n",
      "daily_afd_log/2023-01-13/2023_January_2.txt\n",
      "daily_afd_log/2023-01-14/2023_January_3.txt\n",
      "daily_afd_log/2023-01-15/2023_January_4.txt\n",
      "daily_afd_log/2023-01-16/2023_January_5.txt\n",
      "daily_afd_log/2023-01-17/2023_January_6.txt\n",
      "daily_afd_log/2023-01-18/2023_January_7.txt\n",
      "daily_afd_log/2023-01-19/2023_January_8.txt\n",
      "daily_afd_log/2023-01-20/2023_January_9.txt\n",
      "daily_afd_log/2023-01-21/2023_January_10.txt\n",
      "daily_afd_log/2023-01-21/2023_January_11.txt\n",
      "daily_afd_log/2023-01-21/2023_January_12.txt\n",
      "daily_afd_log/2023-01-21/2023_January_15.txt\n",
      "daily_afd_log/2023-01-21/2023_January_17.txt\n",
      "daily_afd_log/2023-01-21/2023_January_18.txt\n",
      "daily_afd_log/2023-01-21/2023_January_19.txt\n",
      "daily_afd_log/2023-01-21/2023_January_20.txt\n",
      "daily_afd_log/2023-01-22/2023_January_13.txt\n",
      "daily_afd_log/2023-01-22/2023_January_14.txt\n",
      "daily_afd_log/2023-01-22/2023_January_16.txt\n",
      "daily_afd_log/2023-01-22/2023_January_21.txt\n",
      "daily_afd_log/2023-02-13/2023_February_2.txt\n",
      "daily_afd_log/2023-02-21/2023_February_10.txt\n",
      "daily_afd_log/2023-02-22/2023_February_11.txt\n",
      "daily_afd_log/2023-02-23/2023_February_12.txt\n",
      "daily_afd_log/2023-02-24/2023_February_13.txt\n",
      "daily_afd_log/2023-02-25/2023_February_14.txt\n",
      "daily_afd_log/2023-02-26/2023_February_15.txt\n",
      "daily_afd_log/2023-02-27/2023_February_16.txt\n",
      "daily_afd_log/2023-02-28/2023_February_17.txt\n",
      "daily_afd_log/2023-03-01/2023_February_18.txt\n",
      "daily_afd_log/2023-03-02/2023_February_19.txt\n",
      "daily_afd_log/2023-03-03/2023_February_20.txt\n",
      "daily_afd_log/2023-03-04/2023_February_21.txt\n",
      "daily_afd_log/2023-03-05/2023_February_22.txt\n",
      "daily_afd_log/2023-03-06/2023_February_23.txt\n",
      "daily_afd_log/2023-03-07/2023_February_24.txt\n",
      "daily_afd_log/2023-03-08/2023_February_25.txt\n",
      "daily_afd_log/2023-03-09/2023_February_26.txt\n",
      "daily_afd_log/2023-03-10/2023_February_27.txt\n",
      "daily_afd_log/2023-03-11/2023_February_28.txt\n",
      "daily_afd_log/2023-03-11/2023_March_1.txt\n",
      "daily_afd_log/2023-03-11/2023_March_10.txt\n",
      "daily_afd_log/2023-03-11/2023_March_11.txt\n",
      "daily_afd_log/2023-03-11/2023_March_2.txt\n",
      "daily_afd_log/2023-03-11/2023_March_3.txt\n",
      "daily_afd_log/2023-03-11/2023_March_4.txt\n",
      "daily_afd_log/2023-03-11/2023_March_5.txt\n",
      "daily_afd_log/2023-03-11/2023_March_6.txt\n",
      "daily_afd_log/2023-03-11/2023_March_7.txt\n",
      "daily_afd_log/2023-03-11/2023_March_8.txt\n",
      "daily_afd_log/2023-03-11/2023_March_9.txt\n",
      "daily_afd_log/2023-04-30/2023_April_19.txt\n",
      "daily_afd_log/2023-05-01/2023_April_20.txt\n",
      "daily_afd_log/2023-05-02/2023_April_21.txt\n",
      "daily_afd_log/2023-05-03/2023_April_22.txt\n",
      "daily_afd_log/2023-05-04/2023_April_23.txt\n",
      "daily_afd_log/2023-05-07/2023_April_24.txt\n",
      "daily_afd_log/2023-05-07/2023_April_25.txt\n",
      "daily_afd_log/2023-05-07/2023_April_26.txt\n",
      "daily_afd_log/2023-05-08/2023_April_27.txt\n",
      "daily_afd_log/2023-05-09/2023_April_28.txt\n",
      "daily_afd_log/2023-05-11/2023_April_30.txt\n",
      "daily_afd_log/2023-05-12/2023_April_29.txt\n",
      "daily_afd_log/2023-05-12/2023_May_1.txt\n",
      "daily_afd_log/2023-05-13/2023_May_2.txt\n",
      "daily_afd_log/2023-05-15/2023_May_4.txt\n",
      "daily_afd_log/2023-05-16/2023_May_3.txt\n",
      "daily_afd_log/2023-05-16/2023_May_5.txt\n",
      "daily_afd_log/2023-05-19/2023_May_6.txt\n",
      "daily_afd_log/2023-05-19/2023_May_7.txt\n",
      "daily_afd_log/2023-05-20/2023_May_8.txt\n",
      "daily_afd_log/2023-05-22/2023_May_10.txt\n",
      "daily_afd_log/2023-05-22/2023_May_11.txt\n",
      "daily_afd_log/2023-05-22/2023_May_12.txt\n",
      "daily_afd_log/2023-05-22/2023_May_9.txt\n",
      "daily_afd_log/2023-05-27/2023_May_13.txt\n",
      "daily_afd_log/2023-05-27/2023_May_14.txt\n",
      "daily_afd_log/2023-05-27/2023_May_15.txt\n",
      "daily_afd_log/2023-05-27/2023_May_16.txt\n",
      "daily_afd_log/2023-05-27/2023_May_17.txt\n",
      "daily_afd_log/2023-05-27/2023_May_18.txt\n",
      "daily_afd_log/2023-05-27/2023_May_19.txt\n",
      "daily_afd_log/2023-05-27/2023_May_20.txt\n",
      "daily_afd_log/2023-05-27/2023_May_21.txt\n",
      "daily_afd_log/2023-05-27/2023_May_22.txt\n",
      "daily_afd_log/2023-05-27/2023_May_23.txt\n",
      "daily_afd_log/2023-05-27/2023_May_24.txt\n",
      "daily_afd_log/2023-05-27/2023_May_25.txt\n",
      "daily_afd_log/2023-05-27/2023_May_26.txt\n",
      "daily_afd_log/2023-05-27/2023_May_27.txt\n"
     ]
    }
   ],
   "source": [
    "people_metadata = extract_people_metadata_from_logs(objects_pd, INPUT_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>found_person</th>\n",
       "      <th>num_entities</th>\n",
       "      <th>is_multiple_entity_types</th>\n",
       "      <th>file_name</th>\n",
       "      <th>discussion</th>\n",
       "      <th>afd_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Margaret Louise Skourlis</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_21.txt</td>\n",
       "      <td>&lt;div class=\"boilerplate afd vfd xfd-closed arc...</td>\n",
       "      <td>delete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Featherston Drive Public School</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_21.txt</td>\n",
       "      <td>&lt;div class=\"boilerplate afd vfd xfd-closed arc...</td>\n",
       "      <td>delete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Michael D. Mehta</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_21.txt</td>\n",
       "      <td>&lt;div class=\"boilerplate afd vfd xfd-closed arc...</td>\n",
       "      <td>delete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Index of World War II articles</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_21.txt</td>\n",
       "      <td>&lt;div class=\"boilerplate afd vfd xfd-closed arc...</td>\n",
       "      <td>delete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Radical love (social psychology)</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_21.txt</td>\n",
       "      <td>&lt;div class=\"boilerplate afd vfd xfd-closed arc...</td>\n",
       "      <td>soft delete</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             entity  found_person  num_entities  \\\n",
       "0          Margaret Louise Skourlis          True             1   \n",
       "0   Featherston Drive Public School         False             1   \n",
       "0                  Michael D. Mehta          True             1   \n",
       "0    Index of World War II articles         False             1   \n",
       "0  Radical love (social psychology)         False             0   \n",
       "\n",
       "   is_multiple_entity_types                                      file_name  \\\n",
       "0                     False  daily_afd_log/2023-01-01/2022_December_21.txt   \n",
       "0                     False  daily_afd_log/2023-01-01/2022_December_21.txt   \n",
       "0                     False  daily_afd_log/2023-01-01/2022_December_21.txt   \n",
       "0                     False  daily_afd_log/2023-01-01/2022_December_21.txt   \n",
       "0                     False  daily_afd_log/2023-01-01/2022_December_21.txt   \n",
       "\n",
       "                                          discussion   afd_result  \n",
       "0  <div class=\"boilerplate afd vfd xfd-closed arc...       delete  \n",
       "0  <div class=\"boilerplate afd vfd xfd-closed arc...       delete  \n",
       "0  <div class=\"boilerplate afd vfd xfd-closed arc...       delete  \n",
       "0  <div class=\"boilerplate afd vfd xfd-closed arc...       delete  \n",
       "0  <div class=\"boilerplate afd vfd xfd-closed arc...  soft delete  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_metadata[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2107, 7)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "delete                    938\n",
       "keep                      370\n",
       "soft delete               222\n",
       "redirect                  209\n",
       "merge                     104\n",
       "no consensus               96\n",
       "speedy keep                61\n",
       "speedy delete              30\n",
       "draftify                   27\n",
       "withdrawn                   9\n",
       "delete and redirect         5\n",
       "speedy deleted              3\n",
       "procedural close            2\n",
       "procedural keep             2\n",
       "iar draftify                1\n",
       "userfy and delete           1\n",
       "speedy delete and salt      1\n",
       "keep but rename             1\n",
       "soft redirect               1\n",
       "weak keep                   1\n",
       "soft keep                   1\n",
       "wrong venue                 1\n",
       "speedy deleted as g11       1\n",
       "withdrawn by nominator      1\n",
       "deleted bc socking          1\n",
       "Name: afd_result, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_metadata['afd_result'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_writer = boto3.client('s3',\n",
    "                    region_name='us-east-1',\n",
    "                    aws_access_key_id=cfg.aws_writer['accessCode'],\n",
    "                    aws_secret_access_key=cfg.aws_writer['secretCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'NW277YAGATAJZW0C',\n",
       "  'HostId': 'WE6niZhZ2DasTYmhDPDYzThDRyW8Ti2Ql6avAoAgsSmvFdb+9uxjpRlbiCMe6xSYDxYm+e/Brfg=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'WE6niZhZ2DasTYmhDPDYzThDRyW8Ti2Ql6avAoAgsSmvFdb+9uxjpRlbiCMe6xSYDxYm+e/Brfg=',\n",
       "   'x-amz-request-id': 'NW277YAGATAJZW0C',\n",
       "   'date': 'Sat, 27 May 2023 15:46:28 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"d9258d20f1351923a536016cf1567655\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"d9258d20f1351923a536016cf1567655\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_buffer = io.BytesIO()\n",
    "people_metadata.to_parquet(out_buffer, index=False)\n",
    "s3_writer.put_object( Bucket=OUTPUT_BUCKET, Key=OUTPUT_FILE, Body=out_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
