{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives: \n",
    "1) Gather names of people whose Wikipedia articles have been nominated for deletion\n",
    "\n",
    "2) Gather free text discussion about why those articles were nominated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "from bs4 import BeautifulSoup\n",
    "import boto3\n",
    "import config as cfg\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_BUCKET = 'afd-scraped'\n",
    "PREFIX = \"daily_afd_log/2023\"\n",
    "OUTPUT_BUCKET = 'women-in-red-intermediary'\n",
    "OUTPUT_FILE = 'afd_names_and_discussion.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3',\n",
    "                    region_name='us-east-1',\n",
    "                    aws_access_key_id=cfg.aws_reader['accessCode'],\n",
    "                    aws_secret_access_key=cfg.aws_reader['secretCode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_h3_containers(bs4_result_set):\n",
    "    \"\"\"\n",
    "    Extracts the text and tags of the specified type between h3 tags in a BeautifulSoup result set.\n",
    "\n",
    "    Args:\n",
    "        bs4_result_set (bs4.element.ResultSet): A result set containing h3 tags.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of h3 tag names and their contents, where each value is a list of\n",
    "              BeautifulSoup tags representing the contents between the corresponding h3 tags.\n",
    "\n",
    "    Example:\n",
    "        >>> h3_tags = soup.find_all(\"h3\")\n",
    "        >>> h3_containers = create_h3_containers(h3_tags)\n",
    "        >>> print(h3_containers[\"Section 1\"])\n",
    "        [<p>Some text</p>, <ul><li>Item 1</li><li>Item 2</li></ul>, <p>More text</p>]\n",
    "    \"\"\"\n",
    "\n",
    "    h3_containers = {}\n",
    "    TEXT_TO_REMOVE = \"[edit]\"\n",
    "\n",
    "    for i, current_h3 in enumerate(bs4_result_set):\n",
    "        next_h3 = bs4_result_set[i + 1] if i < len(bs4_result_set) - 1 else None\n",
    "        this_tag_sibling = [sibling for sibling in current_h3.next_siblings if sibling != next_h3]\n",
    "        clean_h3_name = current_h3.text.replace(TEXT_TO_REMOVE, \"\")\n",
    "        h3_containers[clean_h3_name] = this_tag_sibling\n",
    "\n",
    "    return h3_containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_afd_result_from_containers(bs4_result_set):\n",
    "    \"\"\"\n",
    "    Extracts the text and tags of the specified type between h3 tags in a BeautifulSoup result set.\n",
    "\n",
    "    Args:\n",
    "        bs4_result_set (bs4.element.ResultSet): A result set containing h3 tags.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of h3 tag names and the preceding <p> tag, which provides the result of the \n",
    "        articles for deletion discussion\n",
    "\n",
    "    Example:\n",
    "        >>> h3_tags = soup.find_all(\"h3\")\n",
    "        >>> h3_containers = create_h3_containers(h3_tags)\n",
    "        >>> print(h3_containers[\"Section 1\"])\n",
    "        [<p>The result was delete</p>]\n",
    "    \"\"\"\n",
    "\n",
    "    h3_containers = {}\n",
    "    TEXT_TO_REMOVE = \"[edit]\"\n",
    "\n",
    "    for h3_tag in bs4_result_set:\n",
    " \n",
    "        p_tag = h3_tag.find_previous_sibling('p')\n",
    "        if p_tag:\n",
    "            clean_h3_name = h3_tag.text.replace(TEXT_TO_REMOVE, \"\")\n",
    "            h3_containers[clean_h3_name] = p_tag\n",
    "        \n",
    "    return h3_containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_people_metadata_from_logs(logs_df, s3_bucket):\n",
    "    '''\n",
    "    Given a Pandas DataFrame of log files and an S3 bucket name, \n",
    "    extracts metadata about people mentioned in the logs.\n",
    "    \n",
    "    :param logs_df: A Pandas DataFrame containing a column of S3 object keys.\n",
    "    :type logs_df: pandas.DataFrame\n",
    "    :param s3_bucket: The name of the S3 bucket containing the log files.\n",
    "    :type s3_bucket: str\n",
    "    :return: A Pandas DataFrame containing metadata about people mentioned in the logs.\n",
    "    :rtype: pandas.DataFrame\n",
    "    '''\n",
    "    \n",
    "    people_metadata = None\n",
    "\n",
    "    for file in logs_df['file_name'].values:\n",
    "        print(file)\n",
    "\n",
    "        object_content = read_s3_file(s3_bucket, file)\n",
    "        h3_tags = parse_h3_tags(object_content)\n",
    "        h3_content = create_h3_containers(h3_tags)\n",
    "        afd_result = get_afd_result_from_containers(h3_tags)\n",
    "\n",
    "        for name in h3_content.keys():\n",
    "            results = pd.DataFrame(identify_people(name), index=[0])\n",
    "            results['file_name'] = file\n",
    "            results['discussion'] = ''.join([str(x) for x in h3_content.get(name)]) # flatten list of content between h3 stags\n",
    "            \n",
    "            try:\n",
    "                results['afd_result'] = str(afd_result.get(name))\n",
    "            except:\n",
    "                results['afd_result'] = None\n",
    "            \n",
    "            if people_metadata is None:\n",
    "                people_metadata = results\n",
    "            else:\n",
    "                people_metadata = pd.concat([people_metadata, results])\n",
    "                \n",
    "    return people_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_s3_files(BUCKET, PREFIX):\n",
    "    \"\"\"\n",
    "    This function takes in the name of an Amazon S3 bucket and a prefix for an S3 key \n",
    "    and returns a list of S3 object keys that match the given prefix.\n",
    "\n",
    "    :param BUCKET: A string representing the name of an S3 bucket\n",
    "    :type BUCKET: str\n",
    "    :param PREFIX: A string representing the prefix to search for in the S3 objects' keys\n",
    "    :type PREFIX: str\n",
    "\n",
    "    :return: A list of S3 object keys that match the given prefix\n",
    "    :rtype: list[str]\n",
    "    \n",
    "    Example Usage:\n",
    "    >>> s3_files = get_list_of_s3_files('my-s3-bucket', 'path/to/my/files/')\n",
    "    >>> print(s3_files)\n",
    "    ['path/to/my/files/file1.txt', 'path/to/my/files/file2.txt', 'path/to/my/files/file3.txt']\n",
    "    \"\"\"\n",
    "    bucket = s3.Bucket(BUCKET)\n",
    "    objects = bucket.objects.filter(Prefix=PREFIX)\n",
    "    return [obj.key for obj in objects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_h3_tags(html_page):\n",
    "    \"\"\"\n",
    "    Parses an HTML page and returns a list of all the <h3> tags found in it.\n",
    "\n",
    "    Args:\n",
    "        html_page (str): The HTML page to parse.\n",
    "\n",
    "    Returns:\n",
    "        A list of BeautifulSoup Tag objects, each representing an <h3> tag found in the HTML page.\n",
    "\n",
    "    Raises:\n",
    "        None.\n",
    "\n",
    "    Example:\n",
    "        >>> html_page = '<html><body><h1>Title</h1><h3>First article</h3><p>Some text.</p><h3>Second article</h3><p>More text.</p></body></html>'\n",
    "        >>> parse_h3_tags(html_page)\n",
    "        [<h3>First article</h3>, <h3>Second article</h3>]\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "    h3_tags = soup.find_all(\"h3\")\n",
    "    return h3_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_recent_log(object_df):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with the most recent 'scrape_date' for each 'log_date'.\n",
    "\n",
    "    :param object_df: pandas.DataFrame\n",
    "        A DataFrame with columns 'file_name', 'scrape_date', and 'log_date'.\n",
    "    :return: pandas.DataFrame\n",
    "        A DataFrame with columns 'log_date' and 'scrape_date', where each row\n",
    "        corresponds to the most recent 'scrape_date' for each 'log_date' in the input DataFrame.\n",
    "    \"\"\"\n",
    "    max_scrape_date = object_df.groupby('log_date')['scrape_date'].max().reset_index()\n",
    "    return max_scrape_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_people(afd_article_name):\n",
    "    \"\"\"\n",
    "    Identify people in an AFD (Articles for Deletion) article name using Named Entity Recognition (NER) provided by the\n",
    "    spaCy library. Returns a dictionary with information about the identified entities.\n",
    "\n",
    "    :param afd_article_name: The name of the AFD article to analyze.\n",
    "    :type afd_article_name: str\n",
    "    :return: A dictionary with information about the identified entities.\n",
    "    :rtype: dict\n",
    "\n",
    "    The dictionary contains the following keys:\n",
    "        - 'entity': The name of the AFD article that was analyzed.\n",
    "        - 'found_person': A boolean value indicating whether at least one person was identified in the article name.\n",
    "        - 'num_entities': The number of unique entity types identified in the article name.\n",
    "        - 'is_multiple_entity_types': A boolean value indicating whether more than one entity type was identified in the\n",
    "                                       article name.\n",
    "\n",
    "    Example usage:\n",
    "    >>> identify_people(\"Wikipedia:Articles for deletion/John Doe\")\n",
    "    {'entity': 'Wikipedia:Articles for deletion/John Doe',\n",
    "     'found_person': True,\n",
    "     'num_entities': 1,\n",
    "     'is_multiple_entity_types': False}\n",
    "    \"\"\"\n",
    "    doc = nlp(afd_article_name)\n",
    "\n",
    "    FOUND_PERSON = False \n",
    "    MULTIPLE_ENTITY_TYPES = False\n",
    "\n",
    "    unique_entity_labels = len(set([entity.label_ for entity in doc.ents])) \n",
    "\n",
    "    if any(entity.label_==\"PERSON\" for entity in doc.ents):\n",
    "        FOUND_PERSON = True\n",
    "    if unique_entity_labels>1 and any(entity.label_==\"PERSON\" for entity in doc.ents):\n",
    "        MULTIPLE_ENTITY_TYPES = True\n",
    "\n",
    "    return {'entity': afd_article_name, 'found_person': FOUND_PERSON, 'num_entities': unique_entity_labels,\n",
    "            'is_multiple_entity_types': MULTIPLE_ENTITY_TYPES}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_s3_file(bucket_name, file_key):\n",
    "    \"\"\"\n",
    "    Reads the contents of a file stored on S3.\n",
    "\n",
    "    :param bucket_name: The name of the S3 bucket.\n",
    "    :type bucket_name: str\n",
    "    :param file_key: The unique key of the file in the S3 bucket.\n",
    "    :type file_key: str\n",
    "    :return: The contents of the file as a string.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    s3_object = s3.Object(bucket_name, file_key)\n",
    "    object_content = s3_object.get()['Body'].read().decode('utf-8')\n",
    "    return object_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_primary_key(df, primary_key_col):\n",
    "    \"\"\"\n",
    "    Checks if a specified column or set of columns constitutes a primary key for a given pandas dataframe. \n",
    "\n",
    "    :param df: Pandas dataframe to be checked for a primary key.\n",
    "    :type df: pandas.DataFrame\n",
    "    :param primary_key_col: Name or list of names of column(s) to be checked for being a primary key.\n",
    "    :type primary_key_col: str or list[str]\n",
    "    :raises AssertionError: If the specified column or set of columns is not a primary key for the given dataframe.\n",
    "    :return: None\n",
    "\n",
    "    Example Usage:\n",
    "    >>> import pandas as pd\n",
    "    >>> data = {'Name': ['John', 'Alex', 'Mike', 'John'], 'Age': [24, 26, 27, 24], 'Gender': ['M', 'M', 'M', 'M']}\n",
    "    >>> df = pd.DataFrame(data)\n",
    "    >>> test_primary_key(df, 'Name')\n",
    "    AssertionError: Name is not the primary key\n",
    "    \"\"\"\n",
    "    try:\n",
    "        assert any(df[primary_key_col].duplicated())==False \n",
    "    except:\n",
    "        raise AssertionError(f'{primary_key_col} is not the primary key')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect a list of Article for Deletion logs in this bucket with desired prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = get_list_of_s3_files(INPUT_BUCKET, PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_pd = pd.DataFrame({\"file_name\": objects})\n",
    "objects_pd['scrape_date'] = objects_pd['file_name'].apply(lambda x: pd.to_datetime(x.split(\"/\")[1]))\n",
    "objects_pd['log_date'] = objects_pd['file_name'].apply(lambda x: x.split(\"/\")[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>scrape_date</th>\n",
       "      <th>log_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_21.txt</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2022_December_21.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_22.txt</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2022_December_22.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_23.txt</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2022_December_23.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_24.txt</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2022_December_24.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_25.txt</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2022_December_25.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>daily_afd_log/2023-05-01/2023_April_27.txt</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>2023_April_27.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>daily_afd_log/2023-05-01/2023_April_28.txt</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>2023_April_28.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>daily_afd_log/2023-05-01/2023_April_29.txt</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>2023_April_29.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>daily_afd_log/2023-05-01/2023_April_30.txt</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>2023_April_30.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>daily_afd_log/2023-05-01/2023_May_1.txt</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>2023_May_1.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>505 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         file_name scrape_date  \\\n",
       "0    daily_afd_log/2023-01-01/2022_December_21.txt  2023-01-01   \n",
       "1    daily_afd_log/2023-01-01/2022_December_22.txt  2023-01-01   \n",
       "2    daily_afd_log/2023-01-01/2022_December_23.txt  2023-01-01   \n",
       "3    daily_afd_log/2023-01-01/2022_December_24.txt  2023-01-01   \n",
       "4    daily_afd_log/2023-01-01/2022_December_25.txt  2023-01-01   \n",
       "..                                             ...         ...   \n",
       "500     daily_afd_log/2023-05-01/2023_April_27.txt  2023-05-01   \n",
       "501     daily_afd_log/2023-05-01/2023_April_28.txt  2023-05-01   \n",
       "502     daily_afd_log/2023-05-01/2023_April_29.txt  2023-05-01   \n",
       "503     daily_afd_log/2023-05-01/2023_April_30.txt  2023-05-01   \n",
       "504        daily_afd_log/2023-05-01/2023_May_1.txt  2023-05-01   \n",
       "\n",
       "                 log_date  \n",
       "0    2022_December_21.txt  \n",
       "1    2022_December_22.txt  \n",
       "2    2022_December_23.txt  \n",
       "3    2022_December_24.txt  \n",
       "4    2022_December_25.txt  \n",
       "..                    ...  \n",
       "500     2023_April_27.txt  \n",
       "501     2023_April_28.txt  \n",
       "502     2023_April_29.txt  \n",
       "503     2023_April_30.txt  \n",
       "504        2023_May_1.txt  \n",
       "\n",
       "[505 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We captured snapshots of Articles for Deletion logs on multiple dates, so let's filter to the most recent snapshot for a given log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_log = get_most_recent_log(objects_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_pd = objects_pd.merge(most_recent_log, on = ['log_date','scrape_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>scrape_date</th>\n",
       "      <th>log_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_21.txt</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2022_December_21.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daily_afd_log/2023-01-02/2022_December_22.txt</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>2022_December_22.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>daily_afd_log/2023-01-03/2022_December_23.txt</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>2022_December_23.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>daily_afd_log/2023-01-04/2022_December_24.txt</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>2022_December_24.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>daily_afd_log/2023-01-05/2022_December_25.txt</td>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>2022_December_25.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>daily_afd_log/2023-05-01/2023_April_27.txt</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>2023_April_27.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>daily_afd_log/2023-05-01/2023_April_28.txt</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>2023_April_28.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>daily_afd_log/2023-05-01/2023_April_29.txt</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>2023_April_29.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>daily_afd_log/2023-05-01/2023_April_30.txt</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>2023_April_30.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>daily_afd_log/2023-05-01/2023_May_1.txt</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>2023_May_1.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        file_name scrape_date  \\\n",
       "0   daily_afd_log/2023-01-01/2022_December_21.txt  2023-01-01   \n",
       "1   daily_afd_log/2023-01-02/2022_December_22.txt  2023-01-02   \n",
       "2   daily_afd_log/2023-01-03/2022_December_23.txt  2023-01-03   \n",
       "3   daily_afd_log/2023-01-04/2022_December_24.txt  2023-01-04   \n",
       "4   daily_afd_log/2023-01-05/2022_December_25.txt  2023-01-05   \n",
       "..                                            ...         ...   \n",
       "71     daily_afd_log/2023-05-01/2023_April_27.txt  2023-05-01   \n",
       "72     daily_afd_log/2023-05-01/2023_April_28.txt  2023-05-01   \n",
       "73     daily_afd_log/2023-05-01/2023_April_29.txt  2023-05-01   \n",
       "74     daily_afd_log/2023-05-01/2023_April_30.txt  2023-05-01   \n",
       "75        daily_afd_log/2023-05-01/2023_May_1.txt  2023-05-01   \n",
       "\n",
       "                log_date  \n",
       "0   2022_December_21.txt  \n",
       "1   2022_December_22.txt  \n",
       "2   2022_December_23.txt  \n",
       "3   2022_December_24.txt  \n",
       "4   2022_December_25.txt  \n",
       "..                   ...  \n",
       "71     2023_April_27.txt  \n",
       "72     2023_April_28.txt  \n",
       "73     2023_April_29.txt  \n",
       "74     2023_April_30.txt  \n",
       "75        2023_May_1.txt  \n",
       "\n",
       "[76 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_primary_key(objects_pd, 'log_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The H3 tag only contains the name of the person or organization associated with the article nominated for deletion. Let's collect the articles for deletion discussion, found between H3 tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily_afd_log/2023-01-01/2022_December_21.txt\n",
      "daily_afd_log/2023-01-02/2022_December_22.txt\n",
      "daily_afd_log/2023-01-03/2022_December_23.txt\n",
      "daily_afd_log/2023-01-04/2022_December_24.txt\n",
      "daily_afd_log/2023-01-05/2022_December_25.txt\n",
      "daily_afd_log/2023-01-06/2022_December_26.txt\n",
      "daily_afd_log/2023-01-07/2022_December_27.txt\n",
      "daily_afd_log/2023-01-08/2022_December_28.txt\n",
      "daily_afd_log/2023-01-09/2022_December_29.txt\n",
      "daily_afd_log/2023-01-10/2022_December_30.txt\n",
      "daily_afd_log/2023-01-11/2022_December_31.txt\n",
      "daily_afd_log/2023-01-12/2023_January_1.txt\n",
      "daily_afd_log/2023-01-13/2023_January_2.txt\n",
      "daily_afd_log/2023-01-14/2023_January_3.txt\n",
      "daily_afd_log/2023-01-15/2023_January_4.txt\n",
      "daily_afd_log/2023-01-16/2023_January_5.txt\n",
      "daily_afd_log/2023-01-17/2023_January_6.txt\n",
      "daily_afd_log/2023-01-18/2023_January_7.txt\n",
      "daily_afd_log/2023-01-19/2023_January_8.txt\n",
      "daily_afd_log/2023-01-20/2023_January_9.txt\n",
      "daily_afd_log/2023-01-21/2023_January_10.txt\n",
      "daily_afd_log/2023-01-21/2023_January_11.txt\n",
      "daily_afd_log/2023-01-21/2023_January_12.txt\n",
      "daily_afd_log/2023-01-21/2023_January_15.txt\n",
      "daily_afd_log/2023-01-21/2023_January_17.txt\n",
      "daily_afd_log/2023-01-21/2023_January_18.txt\n",
      "daily_afd_log/2023-01-21/2023_January_19.txt\n",
      "daily_afd_log/2023-01-21/2023_January_20.txt\n",
      "daily_afd_log/2023-01-22/2023_January_13.txt\n",
      "daily_afd_log/2023-01-22/2023_January_14.txt\n",
      "daily_afd_log/2023-01-22/2023_January_16.txt\n",
      "daily_afd_log/2023-01-22/2023_January_21.txt\n",
      "daily_afd_log/2023-02-13/2023_February_2.txt\n",
      "daily_afd_log/2023-02-21/2023_February_10.txt\n",
      "daily_afd_log/2023-02-22/2023_February_11.txt\n",
      "daily_afd_log/2023-02-23/2023_February_12.txt\n",
      "daily_afd_log/2023-02-24/2023_February_13.txt\n",
      "daily_afd_log/2023-02-25/2023_February_14.txt\n",
      "daily_afd_log/2023-02-26/2023_February_15.txt\n",
      "daily_afd_log/2023-02-27/2023_February_16.txt\n",
      "daily_afd_log/2023-02-28/2023_February_17.txt\n",
      "daily_afd_log/2023-03-01/2023_February_18.txt\n",
      "daily_afd_log/2023-03-02/2023_February_19.txt\n",
      "daily_afd_log/2023-03-03/2023_February_20.txt\n",
      "daily_afd_log/2023-03-04/2023_February_21.txt\n",
      "daily_afd_log/2023-03-05/2023_February_22.txt\n",
      "daily_afd_log/2023-03-06/2023_February_23.txt\n",
      "daily_afd_log/2023-03-07/2023_February_24.txt\n",
      "daily_afd_log/2023-03-08/2023_February_25.txt\n",
      "daily_afd_log/2023-03-09/2023_February_26.txt\n",
      "daily_afd_log/2023-03-10/2023_February_27.txt\n",
      "daily_afd_log/2023-03-11/2023_February_28.txt\n",
      "daily_afd_log/2023-03-11/2023_March_1.txt\n",
      "daily_afd_log/2023-03-11/2023_March_10.txt\n",
      "daily_afd_log/2023-03-11/2023_March_11.txt\n",
      "daily_afd_log/2023-03-11/2023_March_2.txt\n",
      "daily_afd_log/2023-03-11/2023_March_3.txt\n",
      "daily_afd_log/2023-03-11/2023_March_4.txt\n",
      "daily_afd_log/2023-03-11/2023_March_5.txt\n",
      "daily_afd_log/2023-03-11/2023_March_6.txt\n",
      "daily_afd_log/2023-03-11/2023_March_7.txt\n",
      "daily_afd_log/2023-03-11/2023_March_8.txt\n",
      "daily_afd_log/2023-03-11/2023_March_9.txt\n",
      "daily_afd_log/2023-04-30/2023_April_19.txt\n",
      "daily_afd_log/2023-05-01/2023_April_20.txt\n",
      "daily_afd_log/2023-05-01/2023_April_21.txt\n",
      "daily_afd_log/2023-05-01/2023_April_22.txt\n",
      "daily_afd_log/2023-05-01/2023_April_23.txt\n",
      "daily_afd_log/2023-05-01/2023_April_24.txt\n",
      "daily_afd_log/2023-05-01/2023_April_25.txt\n",
      "daily_afd_log/2023-05-01/2023_April_26.txt\n",
      "daily_afd_log/2023-05-01/2023_April_27.txt\n",
      "daily_afd_log/2023-05-01/2023_April_28.txt\n",
      "daily_afd_log/2023-05-01/2023_April_29.txt\n",
      "daily_afd_log/2023-05-01/2023_April_30.txt\n",
      "daily_afd_log/2023-05-01/2023_May_1.txt\n"
     ]
    }
   ],
   "source": [
    "people_metadata = extract_people_metadata_from_logs(objects_pd, INPUT_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>found_person</th>\n",
       "      <th>num_entities</th>\n",
       "      <th>is_multiple_entity_types</th>\n",
       "      <th>file_name</th>\n",
       "      <th>discussion</th>\n",
       "      <th>afd_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Margaret Louise Skourlis</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_21.txt</td>\n",
       "      <td>\\n&lt;dl&gt;&lt;dd&gt;&lt;span id=\"Margaret_Louise_Skourlis\"&gt;...</td>\n",
       "      <td>&lt;p&gt;The result was &lt;b&gt;delete&lt;/b&gt;. Consider this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Featherston Drive Public School</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_21.txt</td>\n",
       "      <td>\\n&lt;div class=\"other-afds\" style=\"width:33%; bo...</td>\n",
       "      <td>&lt;p&gt;The result was &lt;b&gt;delete&lt;/b&gt;. &lt;span style=\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Michael D. Mehta</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_21.txt</td>\n",
       "      <td>\\n&lt;dl&gt;&lt;dd&gt;&lt;span id=\"Michael_D._Mehta\"&gt;&lt;/span&gt;&lt;...</td>\n",
       "      <td>&lt;p&gt;The result was &lt;b&gt;delete&lt;/b&gt;. ♠&lt;a href=\"/wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Index of World War II articles</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_21.txt</td>\n",
       "      <td>\\n&lt;dl&gt;&lt;dd&gt;&lt;span id=\"Index_of_World_War_II_arti...</td>\n",
       "      <td>&lt;p&gt;The result was &lt;b&gt;delete&lt;/b&gt;. Whether index...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Radical love (social psychology)</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_21.txt</td>\n",
       "      <td>\\n&lt;dl&gt;&lt;dd&gt;&lt;span id=\"Radical_love_(social_psych...</td>\n",
       "      <td>&lt;p&gt;The result was &lt;b&gt;soft delete&lt;/b&gt;. Based on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             entity  found_person  num_entities  \\\n",
       "0          Margaret Louise Skourlis          True             1   \n",
       "0   Featherston Drive Public School         False             1   \n",
       "0                  Michael D. Mehta          True             1   \n",
       "0    Index of World War II articles         False             1   \n",
       "0  Radical love (social psychology)         False             0   \n",
       "\n",
       "   is_multiple_entity_types                                      file_name  \\\n",
       "0                     False  daily_afd_log/2023-01-01/2022_December_21.txt   \n",
       "0                     False  daily_afd_log/2023-01-01/2022_December_21.txt   \n",
       "0                     False  daily_afd_log/2023-01-01/2022_December_21.txt   \n",
       "0                     False  daily_afd_log/2023-01-01/2022_December_21.txt   \n",
       "0                     False  daily_afd_log/2023-01-01/2022_December_21.txt   \n",
       "\n",
       "                                          discussion  \\\n",
       "0  \\n<dl><dd><span id=\"Margaret_Louise_Skourlis\">...   \n",
       "0  \\n<div class=\"other-afds\" style=\"width:33%; bo...   \n",
       "0  \\n<dl><dd><span id=\"Michael_D._Mehta\"></span><...   \n",
       "0  \\n<dl><dd><span id=\"Index_of_World_War_II_arti...   \n",
       "0  \\n<dl><dd><span id=\"Radical_love_(social_psych...   \n",
       "\n",
       "                                          afd_result  \n",
       "0  <p>The result was <b>delete</b>. Consider this...  \n",
       "0  <p>The result was <b>delete</b>. <span style=\"...  \n",
       "0  <p>The result was <b>delete</b>. ♠<a href=\"/wi...  \n",
       "0  <p>The result was <b>delete</b>. Whether index...  \n",
       "0  <p>The result was <b>soft delete</b>. Based on...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_metadata[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3',\n",
    "                    region_name='us-east-1',\n",
    "                    aws_access_key_id=cfg.aws_writer['accessCode'],\n",
    "                    aws_secret_access_key=cfg.aws_writer['secretCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3.Object(bucket_name='women-in-red-intermediary', key='afd_names_and_discussion.csv')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_string = people_metadata.to_csv(index=False)\n",
    "s3.Bucket(OUTPUT_BUCKET).put_object(Key=OUTPUT_FILE, Body=csv_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
