{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives: \n",
    "1) Gather names of people whose Wikipedia articles have been nominated for deletion\n",
    "\n",
    "2) Gather free text discussion about why those articles were nominated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import boto3\n",
    "import config as cfg\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_BUCKET = 'afd-scraped'\n",
    "PREFIX = \"daily_afd_log/2023\"\n",
    "OUTPUT_BUCKET = 'women-in-red-intermediary'\n",
    "OUTPUT_FILE = 'afd_names_and_discussion.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'config' has no attribute 'aws_reader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-b876a6a8efc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m s3 = boto3.resource('s3',\n\u001b[1;32m      2\u001b[0m                     \u001b[0mregion_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'us-east-1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                     \u001b[0maws_access_key_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maws_reader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accessCode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                     aws_secret_access_key=cfg.aws_reader['secretCode'])\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'config' has no attribute 'aws_reader'"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3',\n",
    "                    region_name='us-east-1',\n",
    "                    aws_access_key_id=cfg.aws_reader['accessCode'],\n",
    "                    aws_secret_access_key=cfg.aws_reader['secretCode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_h3_containers(bs4_result_set):\n",
    "    \"\"\"\n",
    "    Extracts the text and tags of the specified type between h3 tags in a BeautifulSoup result set.\n",
    "\n",
    "    Args:\n",
    "        bs4_result_set (bs4.element.ResultSet): A result set containing h3 tags.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of h3 tag names and their contents, where each value is a list of\n",
    "              BeautifulSoup tags representing the contents between the corresponding h3 tags.\n",
    "\n",
    "    Example:\n",
    "        >>> h3_tags = soup.find_all(\"h3\")\n",
    "        >>> h3_containers = create_h3_containers(h3_tags)\n",
    "        >>> print(h3_containers[\"Section 1\"])\n",
    "        [<p>Some text</p>, <ul><li>Item 1</li><li>Item 2</li></ul>, <p>More text</p>]\n",
    "    \"\"\"\n",
    "\n",
    "    h3_containers = {}\n",
    "    TEXT_TO_REMOVE = \"[edit]\"\n",
    "\n",
    "    for i, current_h3 in enumerate(bs4_result_set):\n",
    "        next_h3 = bs4_result_set[i + 1] if i < len(bs4_result_set) - 1 else None\n",
    "        this_tag_sibling = [sibling for sibling in current_h3.next_siblings if sibling != next_h3]\n",
    "        clean_h3_name = current_h3.text.replace(TEXT_TO_REMOVE, \"\")\n",
    "        h3_containers[clean_h3_name] = this_tag_sibling\n",
    "\n",
    "    return h3_containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_people_metadata_from_logs(logs_df, s3_bucket):\n",
    "    '''\n",
    "    Given a Pandas DataFrame of log files and an S3 bucket name, \n",
    "    extracts metadata about people mentioned in the logs.\n",
    "    \n",
    "    :param logs_df: A Pandas DataFrame containing a column of S3 object keys.\n",
    "    :type logs_df: pandas.DataFrame\n",
    "    :param s3_bucket: The name of the S3 bucket containing the log files.\n",
    "    :type s3_bucket: str\n",
    "    :return: A Pandas DataFrame containing metadata about people mentioned in the logs.\n",
    "    :rtype: pandas.DataFrame\n",
    "    '''\n",
    "    \n",
    "    people_metadata = None\n",
    "\n",
    "    for file in logs_df['file_name'].values:\n",
    "        print(file)\n",
    "\n",
    "        object_content = read_s3_file(s3_bucket, file)\n",
    "        h3_content = get_h3_tag_content(object_content)\n",
    "\n",
    "        for name in h3_content.keys():\n",
    "            results = pd.DataFrame(identify_people(name), index=[0])\n",
    "            results['file_name'] = file\n",
    "            \n",
    "            if people_metadata is None:\n",
    "                people_metadata = results\n",
    "            else:\n",
    "                people_metadata = pd.concat([people_metadata, results])\n",
    "                \n",
    "    return people_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_s3_files(BUCKET, PREFIX):\n",
    "    \"\"\"\n",
    "    This function takes in the name of an Amazon S3 bucket and a prefix for an S3 key \n",
    "    and returns a list of S3 object keys that match the given prefix.\n",
    "\n",
    "    :param BUCKET: A string representing the name of an S3 bucket\n",
    "    :type BUCKET: str\n",
    "    :param PREFIX: A string representing the prefix to search for in the S3 objects' keys\n",
    "    :type PREFIX: str\n",
    "\n",
    "    :return: A list of S3 object keys that match the given prefix\n",
    "    :rtype: list[str]\n",
    "    \n",
    "    Example Usage:\n",
    "    >>> s3_files = get_list_of_s3_files('my-s3-bucket', 'path/to/my/files/')\n",
    "    >>> print(s3_files)\n",
    "    ['path/to/my/files/file1.txt', 'path/to/my/files/file2.txt', 'path/to/my/files/file3.txt']\n",
    "    \"\"\"\n",
    "    bucket = s3.Bucket(BUCKET)\n",
    "    objects = bucket.objects.filter(Prefix=PREFIX)\n",
    "    return [obj.key for obj in objects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h3_tag_content(html_page):\n",
    "    '''\n",
    "    Given an HTML page as a string, returns the content of all H3 tags in the page. The content of each H3 tag is\n",
    "    returned in a list. If there are no H3 tags in the page, an empty list is returned.\n",
    "\n",
    "    :param html_page: The HTML content of the page as a string.\n",
    "    :type html_page: str\n",
    "    :return: A list of strings containing the content of all H3 tags in the page.\n",
    "    :rtype: list[str]\n",
    "\n",
    "    Example Usage:\n",
    "    >>> html_page = \"<html><body><h3>Article 1</h3><p>Content of Article 1</p><h3>Article 2</h3><p>Content of Article 2</p></body></html>\"\n",
    "    >>> h3_content = get_h3_tag_content(html_page)\n",
    "    >>> print(h3_content)\n",
    "    ['Article 1', 'Article 2']\n",
    "    '''\n",
    "    \n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "\n",
    "    # The names of individuals/organization articles nominated for deletion are stored in H3 stags\n",
    "    h3_tags = soup.find_all(\"h3\")\n",
    "\n",
    "    return create_h3_containers(h3_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_recent_log(object_df):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with the most recent 'scrape_date' for each 'log_date'.\n",
    "\n",
    "    :param object_df: pandas.DataFrame\n",
    "        A DataFrame with columns 'file_name', 'scrape_date', and 'log_date'.\n",
    "    :return: pandas.DataFrame\n",
    "        A DataFrame with columns 'log_date' and 'scrape_date', where each row\n",
    "        corresponds to the most recent 'scrape_date' for each 'log_date' in the input DataFrame.\n",
    "    \"\"\"\n",
    "    max_scrape_date = object_df.groupby('log_date')['scrape_date'].max().reset_index()\n",
    "    return max_scrape_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_people(afd_article_name):\n",
    "    \"\"\"\n",
    "    Identify people in an AFD (Articles for Deletion) article name using Named Entity Recognition (NER) provided by the\n",
    "    spaCy library. Returns a dictionary with information about the identified entities.\n",
    "\n",
    "    :param afd_article_name: The name of the AFD article to analyze.\n",
    "    :type afd_article_name: str\n",
    "    :return: A dictionary with information about the identified entities.\n",
    "    :rtype: dict\n",
    "\n",
    "    The dictionary contains the following keys:\n",
    "        - 'entity': The name of the AFD article that was analyzed.\n",
    "        - 'found_person': A boolean value indicating whether at least one person was identified in the article name.\n",
    "        - 'num_entities': The number of unique entity types identified in the article name.\n",
    "        - 'is_multiple_entity_types': A boolean value indicating whether more than one entity type was identified in the\n",
    "                                       article name.\n",
    "\n",
    "    Example usage:\n",
    "    >>> identify_people(\"Wikipedia:Articles for deletion/John Doe\")\n",
    "    {'entity': 'Wikipedia:Articles for deletion/John Doe',\n",
    "     'found_person': True,\n",
    "     'num_entities': 1,\n",
    "     'is_multiple_entity_types': False}\n",
    "    \"\"\"\n",
    "    doc = nlp(afd_article_name)\n",
    "\n",
    "    FOUND_PERSON = False \n",
    "    MULTIPLE_ENTITY_TYPES = False\n",
    "\n",
    "    unique_entity_labels = len(set([entity.label_ for entity in doc.ents])) \n",
    "\n",
    "    if any(entity.label_==\"PERSON\" for entity in doc.ents):\n",
    "        FOUND_PERSON = True\n",
    "    if unique_entity_labels>1 and any(entity.label_==\"PERSON\" for entity in doc.ents):\n",
    "        MULTIPLE_ENTITY_TYPES = True\n",
    "\n",
    "    return {'entity': afd_article_name, 'found_person': FOUND_PERSON, 'num_entities': unique_entity_labels,\n",
    "            'is_multiple_entity_types': MULTIPLE_ENTITY_TYPES}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_s3_file(bucket_name, file_key):\n",
    "    \"\"\"\n",
    "    Reads the contents of a file stored on S3.\n",
    "\n",
    "    :param bucket_name: The name of the S3 bucket.\n",
    "    :type bucket_name: str\n",
    "    :param file_key: The unique key of the file in the S3 bucket.\n",
    "    :type file_key: str\n",
    "    :return: The contents of the file as a string.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    s3_object = s3.Object(bucket_name, file_key)\n",
    "    object_content = s3_object.get()['Body'].read().decode('utf-8')\n",
    "    return object_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_primary_key(df, primary_key_col):\n",
    "    \"\"\"\n",
    "    Checks if a specified column or set of columns constitutes a primary key for a given pandas dataframe. \n",
    "\n",
    "    :param df: Pandas dataframe to be checked for a primary key.\n",
    "    :type df: pandas.DataFrame\n",
    "    :param primary_key_col: Name or list of names of column(s) to be checked for being a primary key.\n",
    "    :type primary_key_col: str or list[str]\n",
    "    :raises AssertionError: If the specified column or set of columns is not a primary key for the given dataframe.\n",
    "    :return: None\n",
    "\n",
    "    Example Usage:\n",
    "    >>> import pandas as pd\n",
    "    >>> data = {'Name': ['John', 'Alex', 'Mike', 'John'], 'Age': [24, 26, 27, 24], 'Gender': ['M', 'M', 'M', 'M']}\n",
    "    >>> df = pd.DataFrame(data)\n",
    "    >>> test_primary_key(df, 'Name')\n",
    "    AssertionError: Name is not the primary key\n",
    "    \"\"\"\n",
    "    try:\n",
    "        assert any(df[primary_key_col].duplicated())==False \n",
    "    except:\n",
    "        raise AssertionError(f'{primary_key_col} is not the primary key')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect a list of Article for Deletion logs in this bucket with desired prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = get_list_of_s3_files(INPUT_BUCKET, PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_pd = pd.DataFrame({\"file_name\": objects})\n",
    "objects_pd['scrape_date'] = objects_pd['file_name'].apply(lambda x: pd.to_datetime(x.split(\"/\")[1]))\n",
    "objects_pd['log_date'] = objects_pd['file_name'].apply(lambda x: x.split(\"/\")[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>scrape_date</th>\n",
       "      <th>log_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_21.txt</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2022_December_21.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_22.txt</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2022_December_22.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_23.txt</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2022_December_23.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_24.txt</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2022_December_24.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_25.txt</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2022_December_25.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>daily_afd_log/2023-03-11/2023_March_5.txt</td>\n",
       "      <td>2023-03-11</td>\n",
       "      <td>2023_March_5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>daily_afd_log/2023-03-11/2023_March_6.txt</td>\n",
       "      <td>2023-03-11</td>\n",
       "      <td>2023_March_6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>daily_afd_log/2023-03-11/2023_March_7.txt</td>\n",
       "      <td>2023-03-11</td>\n",
       "      <td>2023_March_7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>daily_afd_log/2023-03-11/2023_March_8.txt</td>\n",
       "      <td>2023-03-11</td>\n",
       "      <td>2023_March_8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>daily_afd_log/2023-03-11/2023_March_9.txt</td>\n",
       "      <td>2023-03-11</td>\n",
       "      <td>2023_March_9.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>481 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         file_name scrape_date  \\\n",
       "0    daily_afd_log/2023-01-01/2022_December_21.txt  2023-01-01   \n",
       "1    daily_afd_log/2023-01-01/2022_December_22.txt  2023-01-01   \n",
       "2    daily_afd_log/2023-01-01/2022_December_23.txt  2023-01-01   \n",
       "3    daily_afd_log/2023-01-01/2022_December_24.txt  2023-01-01   \n",
       "4    daily_afd_log/2023-01-01/2022_December_25.txt  2023-01-01   \n",
       "..                                             ...         ...   \n",
       "476      daily_afd_log/2023-03-11/2023_March_5.txt  2023-03-11   \n",
       "477      daily_afd_log/2023-03-11/2023_March_6.txt  2023-03-11   \n",
       "478      daily_afd_log/2023-03-11/2023_March_7.txt  2023-03-11   \n",
       "479      daily_afd_log/2023-03-11/2023_March_8.txt  2023-03-11   \n",
       "480      daily_afd_log/2023-03-11/2023_March_9.txt  2023-03-11   \n",
       "\n",
       "                 log_date  \n",
       "0    2022_December_21.txt  \n",
       "1    2022_December_22.txt  \n",
       "2    2022_December_23.txt  \n",
       "3    2022_December_24.txt  \n",
       "4    2022_December_25.txt  \n",
       "..                    ...  \n",
       "476      2023_March_5.txt  \n",
       "477      2023_March_6.txt  \n",
       "478      2023_March_7.txt  \n",
       "479      2023_March_8.txt  \n",
       "480      2023_March_9.txt  \n",
       "\n",
       "[481 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We captured snapshots of Articles for Deletion logs on multiple dates, so let's filter to the most recent snapshot for a given log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_log = get_most_recent_log(objects_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_pd = objects_pd.merge(most_recent_log, on = ['log_date','scrape_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>scrape_date</th>\n",
       "      <th>log_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_21.txt</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2022_December_21.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daily_afd_log/2023-01-02/2022_December_22.txt</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>2022_December_22.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>daily_afd_log/2023-01-03/2022_December_23.txt</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>2022_December_23.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>daily_afd_log/2023-01-04/2022_December_24.txt</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>2022_December_24.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>daily_afd_log/2023-01-05/2022_December_25.txt</td>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>2022_December_25.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>daily_afd_log/2023-03-11/2023_March_5.txt</td>\n",
       "      <td>2023-03-11</td>\n",
       "      <td>2023_March_5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>daily_afd_log/2023-03-11/2023_March_6.txt</td>\n",
       "      <td>2023-03-11</td>\n",
       "      <td>2023_March_6.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>daily_afd_log/2023-03-11/2023_March_7.txt</td>\n",
       "      <td>2023-03-11</td>\n",
       "      <td>2023_March_7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>daily_afd_log/2023-03-11/2023_March_8.txt</td>\n",
       "      <td>2023-03-11</td>\n",
       "      <td>2023_March_8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>daily_afd_log/2023-03-11/2023_March_9.txt</td>\n",
       "      <td>2023-03-11</td>\n",
       "      <td>2023_March_9.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        file_name scrape_date  \\\n",
       "0   daily_afd_log/2023-01-01/2022_December_21.txt  2023-01-01   \n",
       "1   daily_afd_log/2023-01-02/2022_December_22.txt  2023-01-02   \n",
       "2   daily_afd_log/2023-01-03/2022_December_23.txt  2023-01-03   \n",
       "3   daily_afd_log/2023-01-04/2022_December_24.txt  2023-01-04   \n",
       "4   daily_afd_log/2023-01-05/2022_December_25.txt  2023-01-05   \n",
       "..                                            ...         ...   \n",
       "58      daily_afd_log/2023-03-11/2023_March_5.txt  2023-03-11   \n",
       "59      daily_afd_log/2023-03-11/2023_March_6.txt  2023-03-11   \n",
       "60      daily_afd_log/2023-03-11/2023_March_7.txt  2023-03-11   \n",
       "61      daily_afd_log/2023-03-11/2023_March_8.txt  2023-03-11   \n",
       "62      daily_afd_log/2023-03-11/2023_March_9.txt  2023-03-11   \n",
       "\n",
       "                log_date  \n",
       "0   2022_December_21.txt  \n",
       "1   2022_December_22.txt  \n",
       "2   2022_December_23.txt  \n",
       "3   2022_December_24.txt  \n",
       "4   2022_December_25.txt  \n",
       "..                   ...  \n",
       "58      2023_March_5.txt  \n",
       "59      2023_March_6.txt  \n",
       "60      2023_March_7.txt  \n",
       "61      2023_March_8.txt  \n",
       "62      2023_March_9.txt  \n",
       "\n",
       "[63 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_primary_key(objects_pd, 'log_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The H3 tag only contains the name of the person or organization associated with the article nominated for deletion. Let's collect the articles for deletion discussion, found between H3 tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily_afd_log/2023-01-01/2022_December_21.txt\n",
      "daily_afd_log/2023-01-02/2022_December_22.txt\n",
      "daily_afd_log/2023-01-03/2022_December_23.txt\n",
      "daily_afd_log/2023-01-04/2022_December_24.txt\n",
      "daily_afd_log/2023-01-05/2022_December_25.txt\n",
      "daily_afd_log/2023-01-06/2022_December_26.txt\n",
      "daily_afd_log/2023-01-07/2022_December_27.txt\n",
      "daily_afd_log/2023-01-08/2022_December_28.txt\n",
      "daily_afd_log/2023-01-09/2022_December_29.txt\n",
      "daily_afd_log/2023-01-10/2022_December_30.txt\n",
      "daily_afd_log/2023-01-11/2022_December_31.txt\n",
      "daily_afd_log/2023-01-12/2023_January_1.txt\n",
      "daily_afd_log/2023-01-13/2023_January_2.txt\n",
      "daily_afd_log/2023-01-14/2023_January_3.txt\n",
      "daily_afd_log/2023-01-15/2023_January_4.txt\n",
      "daily_afd_log/2023-01-16/2023_January_5.txt\n",
      "daily_afd_log/2023-01-17/2023_January_6.txt\n",
      "daily_afd_log/2023-01-18/2023_January_7.txt\n",
      "daily_afd_log/2023-01-19/2023_January_8.txt\n",
      "daily_afd_log/2023-01-20/2023_January_9.txt\n",
      "daily_afd_log/2023-01-21/2023_January_10.txt\n",
      "daily_afd_log/2023-01-21/2023_January_11.txt\n",
      "daily_afd_log/2023-01-21/2023_January_12.txt\n",
      "daily_afd_log/2023-01-21/2023_January_15.txt\n",
      "daily_afd_log/2023-01-21/2023_January_17.txt\n",
      "daily_afd_log/2023-01-21/2023_January_18.txt\n",
      "daily_afd_log/2023-01-21/2023_January_19.txt\n",
      "daily_afd_log/2023-01-21/2023_January_20.txt\n",
      "daily_afd_log/2023-01-22/2023_January_13.txt\n",
      "daily_afd_log/2023-01-22/2023_January_14.txt\n",
      "daily_afd_log/2023-01-22/2023_January_16.txt\n",
      "daily_afd_log/2023-01-22/2023_January_21.txt\n",
      "daily_afd_log/2023-02-13/2023_February_2.txt\n",
      "daily_afd_log/2023-02-21/2023_February_10.txt\n",
      "daily_afd_log/2023-02-22/2023_February_11.txt\n",
      "daily_afd_log/2023-02-23/2023_February_12.txt\n",
      "daily_afd_log/2023-02-24/2023_February_13.txt\n",
      "daily_afd_log/2023-02-25/2023_February_14.txt\n",
      "daily_afd_log/2023-02-26/2023_February_15.txt\n",
      "daily_afd_log/2023-02-27/2023_February_16.txt\n",
      "daily_afd_log/2023-02-28/2023_February_17.txt\n",
      "daily_afd_log/2023-03-01/2023_February_18.txt\n",
      "daily_afd_log/2023-03-02/2023_February_19.txt\n",
      "daily_afd_log/2023-03-03/2023_February_20.txt\n",
      "daily_afd_log/2023-03-04/2023_February_21.txt\n",
      "daily_afd_log/2023-03-05/2023_February_22.txt\n",
      "daily_afd_log/2023-03-06/2023_February_23.txt\n",
      "daily_afd_log/2023-03-07/2023_February_24.txt\n",
      "daily_afd_log/2023-03-08/2023_February_25.txt\n",
      "daily_afd_log/2023-03-09/2023_February_26.txt\n",
      "daily_afd_log/2023-03-10/2023_February_27.txt\n",
      "daily_afd_log/2023-03-11/2023_February_28.txt\n",
      "daily_afd_log/2023-03-11/2023_March_1.txt\n",
      "daily_afd_log/2023-03-11/2023_March_10.txt\n",
      "daily_afd_log/2023-03-11/2023_March_11.txt\n",
      "daily_afd_log/2023-03-11/2023_March_2.txt\n",
      "daily_afd_log/2023-03-11/2023_March_3.txt\n",
      "daily_afd_log/2023-03-11/2023_March_4.txt\n",
      "daily_afd_log/2023-03-11/2023_March_5.txt\n",
      "daily_afd_log/2023-03-11/2023_March_6.txt\n",
      "daily_afd_log/2023-03-11/2023_March_7.txt\n",
      "daily_afd_log/2023-03-11/2023_March_8.txt\n",
      "daily_afd_log/2023-03-11/2023_March_9.txt\n"
     ]
    }
   ],
   "source": [
    "people_metadata = extract_people_metadata_from_logs(objects_pd, INPUT_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>found_person</th>\n",
       "      <th>num_entities</th>\n",
       "      <th>is_multiple_entity_types</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Margaret Louise Skourlis</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_21.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Featherston Drive Public School</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_21.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Michael D. Mehta</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_21.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Index of World War II articles</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_21.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Radical love (social psychology)</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>daily_afd_log/2023-01-01/2022_December_21.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             entity  found_person  num_entities  \\\n",
       "0          Margaret Louise Skourlis          True             1   \n",
       "0   Featherston Drive Public School         False             1   \n",
       "0                  Michael D. Mehta          True             1   \n",
       "0    Index of World War II articles         False             1   \n",
       "0  Radical love (social psychology)         False             0   \n",
       "\n",
       "   is_multiple_entity_types                                      file_name  \n",
       "0                     False  daily_afd_log/2023-01-01/2022_December_21.txt  \n",
       "0                     False  daily_afd_log/2023-01-01/2022_December_21.txt  \n",
       "0                     False  daily_afd_log/2023-01-01/2022_December_21.txt  \n",
       "0                     False  daily_afd_log/2023-01-01/2022_December_21.txt  \n",
       "0                     False  daily_afd_log/2023-01-01/2022_December_21.txt  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_metadata[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3.Object(bucket_name='women-in-red-intermediary', key='afd_names_and_discussion.csv')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_string = people_metadata.to_csv(index=False)\n",
    "\n",
    "s3_writer = boto3.resource('s3',\n",
    "                    region_name='us-east-1',\n",
    "                    aws_access_key_id=cfg.aws_writer['accessCode'],\n",
    "                    aws_secret_access_key=cfg.aws_writer['secretCode'])\n",
    "s3_writer.Bucket(OUTPUT_BUCKET).put_object(Key=OUTPUT_FILE, Body=csv_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
