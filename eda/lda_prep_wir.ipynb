{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective: Pre-process wikipedia articles nominated for deltion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import boto3\n",
    "import config as cfg\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import Phrases\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from pprint import pprint\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../libraries/aws_utils.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../libraries/general_utils.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data_engineering/config.yml', 'r') as file:\n",
    "   config_files = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 5 # number of topics to fit on LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load wiki articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_reader = boto3.resource('s3',\n",
    "                    region_name='us-east-1',\n",
    "                    aws_access_key_id=cfg.aws_reader['accessCode'],\n",
    "                    aws_secret_access_key=cfg.aws_reader['secretCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_to_afd_join_key = read_parquet_file(s3_reader, \n",
    "                                  config_files['INTEREDIARY_OUTPUT_BUCKET'], \n",
    "                      config_files['JOINED_ARTICLE_SCRAPE_DATES_AND_AFD_NAMES'],\n",
    "                                          )\n",
    "test_primary_key(article_to_afd_join_key, ['article_id', 'file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_data = read_parquet_file(s3_reader, \n",
    "                                  config_files['INTEREDIARY_OUTPUT_BUCKET'], \n",
    "                      config_files['INFERRED_GENDER_BY_PRONOUN_COUNT'],\n",
    "                                          )\n",
    "test_primary_key(pronoun_data, ['article_id', 'file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_rows = pronoun_data.shape[0]\n",
    "pronoun_data = pronoun_data.merge(article_to_afd_join_key[['article_id', 'file_name', 'afd_result', 'discussion']],\n",
    "                                on = ['article_id', 'file_name'])\n",
    "assert original_rows == pronoun_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = read_parquet_file(s3_reader, \n",
    "                                  config_files['INTEREDIARY_OUTPUT_BUCKET'], \n",
    "                      config_files['SCRAPED_ARTICLE_TEXT_AND_REFERENCE_TEXT'],\n",
    "                                          )\n",
    "test_primary_key(article_text, ['article_id', 'file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_rows = pronoun_data.shape[0]\n",
    "pronoun_data = pronoun_data.merge(article_text[['article_id', 'file_name', 'articles_text']],\n",
    "                                on = ['article_id', 'file_name'])\n",
    "assert original_rows == pronoun_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>scraped_path</th>\n",
       "      <th>num_male_tokens</th>\n",
       "      <th>num_female_tokens</th>\n",
       "      <th>num_non_binary_tokens</th>\n",
       "      <th>num_neo_tokens</th>\n",
       "      <th>max_pronoun_column</th>\n",
       "      <th>afd_result</th>\n",
       "      <th>discussion</th>\n",
       "      <th>articles_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A.S.D._Villabiagio</td>\n",
       "      <td>daily_afd_log/2023-01-03/2022_December_23.txt</td>\n",
       "      <td>individual_afd_page_html/2023-01-01/A.S.D._Vil...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>keep</td>\n",
       "      <td>&lt;div class=\"boilerplate afd vfd xfd-closed arc...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html class=\"client-nojs\" lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aaron_Kemmer</td>\n",
       "      <td>daily_afd_log/2023-01-19/2023_January_8.txt</td>\n",
       "      <td>individual_afd_page_html/2023-01-01/Aaron_Kemm...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>delete</td>\n",
       "      <td>&lt;div class=\"boilerplate afd vfd xfd-closed arc...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html class=\"client-nojs\" lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abbas_Sajwani</td>\n",
       "      <td>daily_afd_log/2023-01-07/2022_December_27.txt</td>\n",
       "      <td>individual_afd_page_html/2023-01-01/Abbas_Sajw...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>delete</td>\n",
       "      <td>&lt;div class=\"boilerplate afd vfd xfd-closed arc...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html class=\"client-nojs\" lan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           article_id                                      file_name  \\\n",
       "0  A.S.D._Villabiagio  daily_afd_log/2023-01-03/2022_December_23.txt   \n",
       "1        Aaron_Kemmer    daily_afd_log/2023-01-19/2023_January_8.txt   \n",
       "2       Abbas_Sajwani  daily_afd_log/2023-01-07/2022_December_27.txt   \n",
       "\n",
       "                                        scraped_path  num_male_tokens  \\\n",
       "0  individual_afd_page_html/2023-01-01/A.S.D._Vil...                1   \n",
       "1  individual_afd_page_html/2023-01-01/Aaron_Kemm...                5   \n",
       "2  individual_afd_page_html/2023-01-01/Abbas_Sajw...                1   \n",
       "\n",
       "   num_female_tokens  num_non_binary_tokens  num_neo_tokens  \\\n",
       "0                  0                      0               0   \n",
       "1                  0                      2               0   \n",
       "2                  0                      0               0   \n",
       "\n",
       "  max_pronoun_column afd_result  \\\n",
       "0               male       keep   \n",
       "1               male     delete   \n",
       "2               male     delete   \n",
       "\n",
       "                                          discussion  \\\n",
       "0  <div class=\"boilerplate afd vfd xfd-closed arc...   \n",
       "1  <div class=\"boilerplate afd vfd xfd-closed arc...   \n",
       "2  <div class=\"boilerplate afd vfd xfd-closed arc...   \n",
       "\n",
       "                                       articles_text  \n",
       "0  <!DOCTYPE html>\\n<html class=\"client-nojs\" lan...  \n",
       "1  <!DOCTYPE html>\\n<html class=\"client-nojs\" lan...  \n",
       "2  <!DOCTYPE html>\\n<html class=\"client-nojs\" lan...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pronoun_data[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Wikipedia articles\n",
    "* extract text from HTML\n",
    "* remove standard Wikipedia banner messages that are not actual article text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_data['article_soup'] = pronoun_data['articles_text'].apply(lambda x: BeautifulSoup(x, \"html.parser\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_data['article_body'] = pronoun_data['article_soup'].apply(lambda x: x.find_all('div', \n",
    "                                                                                       class_='mw-body-content'))\n",
    "pronoun_data['article_body_text'] = pronoun_data['article_body'].apply(lambda x: x[0].get_text(separator=' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove standard Wikipedia banner messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_afd_warning_element(soup):\n",
    "    '''\n",
    "    Retrieves Articles for Deletion (AFD) warning elements from a BeautifulSoup object.\n",
    "\n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object representing the HTML page.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of AFD warning elements found in the HTML page. Each element is a string.\n",
    "\n",
    "    Description:\n",
    "        This method searches for AFD warning elements within the given BeautifulSoup object. \n",
    "        An AFD warning element indicates that the article is being considered for deletion. \n",
    "        The method looks for <div> elements with the class\n",
    "        'mbox-text-span' and checks if they contain the specific text 'This article is being considered for deletion'.\n",
    "        If a match is found, the warning element is added to the list of found_warnings.\n",
    "\n",
    "        Note:\n",
    "            The returned warning elements may contain additional HTML tags and formatting.\n",
    "\n",
    "    '''\n",
    "    found_warnings = []\n",
    "    possible_afd_warnings = soup.find_all('div', class_ = 'mbox-text-span')\n",
    "    if len(possible_afd_warnings) > 0:\n",
    "        for possible_afd_warning in possible_afd_warnings:\n",
    "            if 'This article is being considered for deletion' in possible_afd_warning.text:\n",
    "                found_warnings = found_warnings + [possible_afd_warning.get_text(separator=' ')]\n",
    "    return found_warnings\n",
    "    \n",
    "def get_notability_warning_element(soup):\n",
    "    '''\n",
    "    Retrieves notability warning elements from a BeautifulSoup object.\n",
    "\n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object representing the HTML page.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of notability warning elements found in the HTML page. Each element is a string.\n",
    "\n",
    "    Description:\n",
    "        This method searches for notability warning elements within the given BeautifulSoup object. \n",
    "        It iterates over a list of classes and a list of specific warning texts. \n",
    "        For each class, it finds <div> elements with that class\n",
    "        and checks if they contain any of the specified warning texts. \n",
    "        If a match is found, the warning element is added\n",
    "        to the list of found_warnings.\n",
    "\n",
    "        Note:\n",
    "            The returned warning elements may contain additional HTML tags and formatting.\n",
    "\n",
    "    '''\n",
    "    classes = ['multiple-issues-text', 'mbox-text-span']\n",
    "    found_warnings = []\n",
    "    \n",
    "    for this_class in classes:\n",
    "        possible_warnings = soup.find_all('div', class_ = this_class)\n",
    "        warnings = ['This article has multiple issues',\n",
    "                    'deletion policy', 'notability guideline', 'nominated for deletion', \n",
    "                    'You can help Wikipedia by expanding it',\n",
    "                   'This article does not cite any sources',\n",
    "                   'improve this article',\n",
    "                   'needs additional citations',\n",
    "                   'Please help improve',\n",
    "                   'link rot', \n",
    "                   'no other articles link to it',\n",
    "                   'The neutrality of this article is disputed']\n",
    "        for warning in warnings:\n",
    "            if len(possible_warnings) > 0:\n",
    "                for possible_warning in possible_warnings:\n",
    "                    if warning.lower() in possible_warning.text.lower():\n",
    "                        found_warnings = found_warnings + [possible_warning.get_text(separator=' ')]\n",
    "    return found_warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_data['afd_warning_element'] = pronoun_data['article_soup'].apply(lambda x: get_afd_warning_element(x) )\n",
    "pronoun_data['notability_warning_element'] = pronoun_data['article_soup'].apply(lambda x: get_notability_warning_element(x) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>afd_warning_element</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[This article is being considered for deletion...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[This article is being considered for deletion...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[This article is being considered for deletion...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[This article is being considered for deletion...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               index  afd_warning_element\n",
       "0                                                 []                  105\n",
       "1  [This article is being considered for deletion...                    2\n",
       "2  [This article is being considered for deletion...                    2\n",
       "3  [This article is being considered for deletion...                    2\n",
       "4  [This article is being considered for deletion...                    2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pronoun_data['afd_warning_element'].value_counts().reset_index()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>notability_warning_element</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[This article is being considered for deletion...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The topic of this article  may not meet Wikip...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[This article is being considered for deletion...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[This article is being considered for deletion...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               index  \\\n",
       "0                                                 []   \n",
       "1  [This article is being considered for deletion...   \n",
       "2  [The topic of this article  may not meet Wikip...   \n",
       "3  [This article is being considered for deletion...   \n",
       "4  [This article is being considered for deletion...   \n",
       "\n",
       "   notability_warning_element  \n",
       "0                          77  \n",
       "1                           2  \n",
       "2                           2  \n",
       "3                           2  \n",
       "4                           2  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pronoun_data['notability_warning_element'].value_counts().reset_index()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_warning(body_text, warning_text):\n",
    "    '''\n",
    "    Removes warning texts from the body text.\n",
    "\n",
    "    Parameters:\n",
    "        body_text (str): The original body text.\n",
    "        warning_text (list): A list of warning texts to be removed from the body text.\n",
    "\n",
    "    Returns:\n",
    "        str: The modified body text with the specified warning texts removed.\n",
    "\n",
    "    Description:\n",
    "        This method removes specific warning texts from the given body text. It iterates over each warning text in the\n",
    "        provided list and uses the `replace()` method to remove each occurrence of the warning text from the body text.\n",
    "        The modified body text is then returned.\n",
    "\n",
    "    Example:\n",
    "        # Original body text\n",
    "        body_text = \"This article has multiple issues. Please help improve it. KEEP\"\n",
    "\n",
    "        # Warning texts to be removed\n",
    "        warning_texts = [\"This article has multiple issues\", \"Please help improve it.\"]\n",
    "\n",
    "        # Remove warning texts\n",
    "        modified_body_text = remove_warning(body_text, warning_texts)\n",
    "\n",
    "        # Print the modified body text\n",
    "        print(modified_body_text)\n",
    "        # Output: \" KEEP\"\n",
    "    '''\n",
    "    try:\n",
    "        for this_warning_text in warning_text:\n",
    "            body_text = body_text.replace(this_warning_text, \"\")\n",
    "        return body_text\n",
    "    except:\n",
    "        return body_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_data['article_body_text_wo_warning'] = pronoun_data[['article_body_text', 'afd_warning_element']].apply(lambda x: remove_warning(x[0], x[1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_data['article_body_text_wo_warning'] = pronoun_data[['article_body_text_wo_warning', 'notability_warning_element']].apply(lambda x: \n",
    "                                                                                                                remove_warning(x[0], x[1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_common_warning_messages(text_to_clean):\n",
    "    '''\n",
    "    Removes warning texts from the body text.\n",
    "\n",
    "    Parameters:\n",
    "        text_to_clean (str): The text to clean\n",
    "\n",
    "    Returns:\n",
    "        str: The modified body text with the specified warning texts removed.\n",
    "\n",
    "    Description:\n",
    "        This method removes specific warning texts from the given body text. It iterates over each warning text in the\n",
    "        provided list and uses the `replace()` method to remove each occurrence of the warning text from the body text.\n",
    "        The modified body text is then returned.\n",
    "    '''\n",
    "    common_warning_text = ['Learn how and when to remove this template message', \n",
    "                          'citation needed']\n",
    "    for substring in common_warning_text:\n",
    "        text_to_clean = text_to_clean.replace(substring, ' ')\n",
    "    return text_to_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_data['article_body_text_wo_common_warning'] = pronoun_data['article_body_text_wo_warning'].apply(lambda x: remove_common_warning_messages(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_data['article_body_text_wo_warning_remove_edit'] = pronoun_data['article_body_text_wo_common_warning'].apply(\n",
    "    lambda x: x.replace(\"[edit]\",\"\").replace(\"\\n\",\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_writer = boto3.client('s3',\n",
    "                    region_name='us-east-1',\n",
    "                    aws_access_key_id=cfg.aws_writer['accessCode'],\n",
    "                    aws_secret_access_key=cfg.aws_writer['secretCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '0AEEP32RJT6AQ7Z0',\n",
       "  'HostId': 'Lyl4vOnDw599/hY/PU1AqF51zp2PfdoowYat2U8ue4Fz0es/QPtlFHLbeLhRbm2nAR9nQEiYO3I=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'Lyl4vOnDw599/hY/PU1AqF51zp2PfdoowYat2U8ue4Fz0es/QPtlFHLbeLhRbm2nAR9nQEiYO3I=',\n",
       "   'x-amz-request-id': '0AEEP32RJT6AQ7Z0',\n",
       "   'date': 'Sun, 16 Jul 2023 16:46:24 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"75a7769aeb7b13f103a3c16e385ecc7f\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"75a7769aeb7b13f103a3c16e385ecc7f\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_buffer = io.BytesIO()\n",
    "pronoun_data.drop(['article_soup','article_body'], axis=1).to_parquet(out_buffer, index=False) # drop a beautiful soup column\n",
    "s3_writer.put_object( Bucket=config_files['INTEREDIARY_OUTPUT_BUCKET'], \n",
    "                     Key=config_files['PREPROCESSED_ARTICLE_TEXT'], \n",
    "                     Body=out_buffer.getvalue())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
